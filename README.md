# Quantifying Cooperation and Allostery in Protein-DNA Complexes Via an Information-Theoretic Approach

We present a method for quantifying internal allosteric networks within proteins using transfer entropy, a statistical measure derived from Schreiber (2000) used for measuring the statistical coherence between time-series. Using the trajectory of alpha carbons in Cartesian Space as time-series data, we extract information regarding how fluctuations originating in one part of a protein cascade throughout the entire complex. More-over, Fluctuations do not always simply dissipate. Cycles of information flow between key residues can be extracted to deduce synergistic interactions throughout the complex.

## Model Systems

A set of 2 protein-DNA complexes were utilized in this study: the gal and lac operons bound to their respective repressors. Fluctuations originating from the hinge-helix region cause a distinct "bend" in the DNA. Molecular dynamics (MD) simulations were performed and Gaussian Network Models (GNM) were calculated for these complexes and their mis-matches (gal repressor bound to lac operon and vice versa). 

## Transfer Entropy

Transfer entropy, a information-theoretic measure, is an alternative to Granger causality, to measure how much uncertainty the past of one time-series (the source) reducing about the past of another time series (the destination). This is done by conditioning on histories of the past and destination time-series. Essentially, if the past source and destination time series reduces more information than the past destination time series, then the source is said the "influence" or "granger-cause" the destination time series. As an alternative Granger causality, transfer entropy is viable for application to non-linear systems. Originally for discrete data, the transfer entropy has been adapted to estimate influences present in continuous data using various estimation methods such as histogram binning, kernel destination, and k nearest neighbors (knn) estimation. Here, we focus on the knn approach first proposed by Kraskov, Stogbauer, and Grassberger (KSG), as it is naturally bias-canceling and has been shown to reduce resulting variance.

## Normalized Transfer Entropy

Classically, transfer entropy estimates dictate how many bits or nats (as dictated by the base of the logarithm) of information the source variable reduces about the destination variable. Since different systems are subject to different dynamics, they may differ in the amount of entropy which can be reduced. Consequently, if one wants to quantitatively compare estimates between systems, it is necessary to normalize transfer entropy estimates. In the discrete sense this can be done by either dividng by the entropy of the destination variable or by dividing the conditional entropy of the destination given its past. Since KSG transfer entropy takes a nearest neighbors approach, normalization is much more difficult. We propose to normalize the transfer entropy by feature-scaling it between a maximum and minimum theoretical transfer entropy. These extremes are obtained using a cardinality approach that optimizes the number of points in the respective subspaces in the full joint space containing the future and past destination variables and the past source variable (For more information, [click here!](https://github.com/benjaminAhlbrecht/ProteinDNAResearch/files/6300235/NormalizedTransferEntropy.pdf)).

## Constructing Information Flow Networks

Since transfer entropy quantifies how much uncertainty (entropy) one variable reduces in another variable, it is often referred to a measure of "information flow" between systems. When calculating information flow in large networks (such as proteins), one can simply calculate the transfer entropy between each pair of nodes in the network, creating a pairwise transfer entropy matrix. Additionally, to determine the direction of information flow between two nodes, it is natural to simply subtract the information flow between the two opposing directions. This is equivalent to subtracting the transposed pairwise matrix from its original.

